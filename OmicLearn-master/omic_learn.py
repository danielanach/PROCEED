"""OmicLearn main file."""
import random
import warnings
import pandas as pd
from PIL import Image
import streamlit as st
from datetime import datetime
warnings.simplefilter("ignore", FutureWarning)

# Session state
import utils.session_states as session_states

# ML functionalities
from utils.ml_helper import perform_cross_validation, transform_dataset, calculate_cm

# Plotting
from utils.plot_helper import (plot_confusion_matrices, plot_feature_importance,
                             plot_pr_curve_cv, plot_roc_curve_cv, perform_EDA)

# UI components and others func.
from utils.ui_helper import (main_components, get_system_report, save_sessions,
                             load_data, main_text_and_data_upload, objdict,
                             generate_sidebar_elements, get_download_link,
                             generate_text, generate_footer_parts)

# Set the configs
APP_TITLE = "Predict SEQ Performance"
st.set_page_config(
    page_title = APP_TITLE,
    page_icon = Image.open('./utils/omic_learn.ico'),
    layout = "centered",
    initial_sidebar_state = "auto")
icon = Image.open('./utils/omic_learn.png')
report = get_system_report()

# This needs to be here as it needs to be after setting ithe initial_sidebar_state
try:
    import xgboost
except ModuleNotFoundError:
    st.warning('**WARNING:** Xgboost not installed. To use xgboost install using `conda install py-xgboost`')


# Choosing sample dataset and data parameter selections
def checkpoint_for_data_upload(state, record_widgets):
    multiselect = record_widgets.multiselect
    state['n_missing'] = state.df.isnull().sum().sum()

    if len(state.df) > 0:
        if state.n_missing > 0:
            st.info(f'**INFO:** Found {state.n_missing} missing values. '
                       'Use missing value imputation or `xgboost` classifier.')
        # Distinguish the features from others
        state['proteins'] = [_ for _ in state.df.columns.to_list() if _[0] != '_']
        state['not_proteins'] = [_ for _ in state.df.columns.to_list() if _[0] == '_']

        # Dataset -- Subset
        # with st.beta_expander("Create subset"):
        #     st.markdown("""
        #                 This section allows you to specify a subset of data based on values within a comma.
        #                 Hence, you can exclude data that should not be used at all.""")
        #     state['subset_column'] = st.selectbox("Select subset column:", ['None'] + state.not_proteins)

        #     if state.subset_column != 'None':
        #         subset_options = state.df[state.subset_column].value_counts().index.tolist()
        #         subset_class = multiselect("Select values to keep:", subset_options, default=subset_options)
        #         state['df_sub'] = state.df[state.df[state.subset_column].isin(subset_class)].copy()
        #     elif state.subset_column == 'None':
        #         state['df_sub'] = state.df.copy()
        #         state['subset_column'] = 'None'

        # Dataset -- Feature selections
        # with st.beta_expander("Classification target (*Required)"):
        #     st.markdown("""
        #         Classification target refers to the column that contains the variables that are used two distinguish the two classes.
        #         In the next section, the unique values of this column can be used to define the two classes.
        #     """)
        #     state['target_column'] = st.selectbox("Select target column:", [""] + state.not_proteins,
        #                                 format_func=lambda x: "Select a classification target" if x == "" else x)
        #     if state.target_column == "":
        #         unique_elements_lst = []
        #     else:
        #         st.markdown(f"Unique elements in `{state.target_column}` column:")
        #         unique_elements = state.df_sub[state.target_column].value_counts()
        #         st.write(unique_elements)
        #         unique_elements_lst = unique_elements.index.tolist()

        # Dataset -- Class definitions
        # with st.beta_expander("Define classes (*Required)"):
        #     st.markdown(f"""
        #         For a binary classification task, one needs to define two classes based on the
        #         unique values in the `{state.target_column}` task column.
        #         It is possible to assign multiple values for each class.
        #     """)
        #     state['class_0'] = multiselect("Select Class 0:", unique_elements_lst, default=None)
        #     state['class_1'] = multiselect("Select Class 1:",
        #                                 [_ for _ in unique_elements_lst if _ not in state.class_0], default=None)
        #     state['remainder'] = [_ for _ in state.not_proteins if _ is not state.target_column]

        # # Once both classes are defined
        # if state.class_0 and state.class_1:

        #     # EDA Part
        #     with st.beta_expander("EDA â€” Exploratory data analysis (^Recommended)"):
        #         st.markdown("""
        #             Use exploratory data anlysis on your dateset to identify potential correlations and biases.
        #             For more information, please visit
        #             [the dedicated Wiki page](https://github.com/OmicEra/OmicLearn/wiki/METHODS-%7C-3.-Exploratory-data-analysis).
        #             """)
        #         state['df_sub_y'] = state.df_sub[state.target_column].isin(state.class_0)
        #         state['eda_method'] = st.selectbox("Select an EDA method:", ["None", "PCA", "Hierarchical clustering"])

        #         if (state.eda_method == "PCA") and (len(state.proteins) < 6):
        #             state['pca_show_features'] = st.checkbox("Show the feature attributes on the graph", value=False)

        #         if (state.eda_method == "Hierarchical clustering"):
        #             state['data_range'] = st.slider("Data range to be visualized",
        #                 0, len(state.proteins), (0, round(len(state.proteins) / 2)), step=3,
        #                 help='In large datasets, it is not possible to visaulize all the features.')

        #         if (state.eda_method != "None") and (st.button('Generate plot', key='eda_run')):
        #             with st.spinner(f"Performing {state.eda_method}.."):
        #                 p = perform_EDA(state)
        #                 st.plotly_chart(p, use_container_width=True)
        #                 get_download_link(p, f"{state.eda_method}.pdf")
        #                 get_download_link(p, f"{state.eda_method}.svg")

        #     with st.beta_expander("Additional features"):
        #         st.markdown("Select additional features. All non numerical values will be encoded (e.g. M/F -> 0,1)")
        #         state['additional_features'] = multiselect("Select additional features for trainig:", state.remainder, default=None)

        #     # Exclude features
        #     with st.beta_expander("Exclude features"):
        #         state['exclude_features'] = []
        #         st.markdown("Exclude some features from the model training by selecting or uploading a CSV file. "
        #                     "This can be useful when, e.g., re-running a model without a top feature and assessing the difference in classification accuracy.")
        #         # File uploading target_column for exclusion
        #         exclusion_file_buffer = st.file_uploader("Upload your CSV (comma(,) seperated) file here in which each row corresponds to a feature to be excluded.", type=["csv"])
        #         exclusion_df, exc_df_warnings = load_data(exclusion_file_buffer, "Comma (,)")
        #         for warning in exc_df_warnings:
        #             st.warning(warning)

        #         if len(exclusion_df) > 0:
        #             st.markdown("The following features will be excluded:")
        #             st.write(exclusion_df)
        #             exclusion_df_list = list(exclusion_df.iloc[:, 0].unique())
        #             state['exclude_features'] = multiselect("Select features to be excluded:",
        #                                             state.proteins, default=exclusion_df_list)
        #         else:
        #             state['exclude_features'] = multiselect("Select features to be excluded:",
        #                                                         state.proteins, default=[])

        #     # Manual feature selection
        #     with st.beta_expander("Manually select features"):
        #         st.markdown("Manually select a subset of features. If only these features should be used, additionally set the "
        #                     "`Feature selection` method to `None`. Otherwise, feature selection will be applied, and only a subset of the manually selected features is used.")
        #         manual_users_features = multiselect("Select your features manually:", state.proteins, default=None)
        #     if manual_users_features:
        #         state.proteins = manual_users_features

        # # Dataset -- Cohort selections
        # with st.beta_expander("Cohort comparison"):
        #     st.markdown('Select cohort column to train on one and predict on another:')
        #     not_proteins_excluded_target_option = state.not_proteins
        #     if state.target_column != "":
        #         not_proteins_excluded_target_option.remove(state.target_column)
        #     state['cohort_column'] = st.selectbox("Select cohort column:", [None] + not_proteins_excluded_target_option)
        #     if state['cohort_column'] == None:
        #         state['cohort_checkbox'] = None
        #     else:
        #         state['cohort_checkbox'] = "Yes"

        #     if 'exclude_features' not in state:
        #         state['exclude_features'] = []

        # state['proteins'] = [_ for _ in state.proteins if _ not in state.exclude_features]

    return state

# Display results and plots
def classify_and_plot(state):

    state.bar = st.progress(0)
    # Cross-Validation
    st.markdown("Performing analysis and Running cross-validation")
    cv_results, cv_curves = perform_cross_validation(state)

    st.header('Cross-validation results')

    top_features = []
    # Feature importances from the classifier
    with st.beta_expander("Feature importances from the classifier"):
        st.subheader('Feature importances from the classifier')
        if state.cv_method == 'RepeatedStratifiedKFold':
            st.markdown(f'This is the average feature importance from all {state.cv_splits*state.cv_repeats} cross validation runs.')
        else:
            st.markdown(f'This is the average feature importance from all {state.cv_splits} cross validation runs.')



        if cv_curves['feature_importances_'] is not None:

            # Check whether all feature importance attributes are 0 or not
            if pd.DataFrame(cv_curves['feature_importances_']).isin([0]).all().all() == False:
                p, feature_df, feature_df_wo_links = plot_feature_importance(cv_curves['feature_importances_'])
                st.plotly_chart(p, use_container_width=True)
                if p:
                    get_download_link(p, 'clf_feature_importance.pdf')
                    get_download_link(p, 'clf_feature_importance.svg')

                # Display `feature_df` with NCBI links
                st.subheader("Feature importances from classifier table")
                st.write(feature_df.to_html(escape=False, index=False), unsafe_allow_html=True)
                get_download_link(feature_df_wo_links, 'clf_feature_importances.csv')

                top_features = feature_df.index.to_list()
            else:
                st.info("All feature importance attribute are zero (0). The plot and table are not displayed.")
        else:
            st.info('Feature importance attribute is not implemented for this classifier.')
    state['top_features'] = top_features
    # ROC-AUC
    with st.beta_expander("Receiver operating characteristic Curve and Precision-Recall Curve"):
        st.subheader('Receiver operating characteristic')
        p = plot_roc_curve_cv(cv_curves['roc_curves_'])
        st.plotly_chart(p, use_container_width=True)
        if p:
            get_download_link(p, 'roc_curve.pdf')
            get_download_link(p, 'roc_curve.svg')

        # Precision-Recall Curve
        st.subheader('Precision-Recall Curve')
        st.markdown("Precision-Recall (PR) Curve might be used for imbalanced datasets.")
        p = plot_pr_curve_cv(cv_curves['pr_curves_'], cv_results['class_ratio_test'])
        st.plotly_chart(p, use_container_width=True)
        if p:
            get_download_link(p, 'pr_curve.pdf')
            get_download_link(p, 'pr_curve.svg')

    # Confusion Matrix (CM)
    with st.beta_expander("Confusion matrix"):
        names = ['CV_split {}'.format(_+1) for _ in range(len(cv_curves['y_hats_']))]
        names.insert(0, 'Sum of all splits')
        p = plot_confusion_matrices(state.class_0, state.class_1, cv_curves['y_hats_'], names)
        st.plotly_chart(p, use_container_width=True)
        if p:
            get_download_link(p, 'cm.pdf')
            get_download_link(p, 'cm.svg')

        cm_results = [calculate_cm(*_)[1] for _ in cv_curves['y_hats_']]

        cm_results = pd.DataFrame(cm_results, columns=['TPR','FPR','TNR','FNR'])
        #(tpr, fpr, tnr, fnr)
        cm_results_ = cm_results.mean().to_frame()
        cm_results_.columns = ['Mean']

        cm_results_['Std'] = cm_results.std()

        st.write("Average peformance for all splits:")
        st.write(cm_results_)

    # Results table
    with st.beta_expander("Table for run results"):
        st.subheader(f'Run results for `{state.classifier}`')
        state['summary'] = pd.DataFrame(pd.DataFrame(cv_results).describe())
        st.write(state.summary)
        st.info("""
            **Info:** `Mean precision` and `Mean recall` values provided in the table above
            are calculated as the mean of all individual splits shown in the confusion matrix,
            not the "Sum of all splits" matrix.
            """)
        get_download_link(state.summary, "run_results.csv")

    if state.cohort_checkbox:
        st.header('Cohort comparison results')
        cohort_results, cohort_curves = perform_cross_validation(state, state.cohort_column)

        with st.beta_expander("Receiver operating characteristic Curve and Precision-Recall Curve"):
            # ROC-AUC for Cohorts
            st.subheader('Receiver operating characteristic')
            p = plot_roc_curve_cv(cohort_curves['roc_curves_'], cohort_curves['cohort_combos'])
            st.plotly_chart(p, use_container_width=True)
            if p:
                get_download_link(p, 'roc_curve_cohort.pdf')
                get_download_link(p, 'roc_curve_cohort.svg')

            # PR Curve for Cohorts
            st.subheader('Precision-Recall Curve')
            st.markdown("Precision-Recall (PR) Curve might be used for imbalanced datasets.")
            p = plot_pr_curve_cv(cohort_curves['pr_curves_'], cohort_results['class_ratio_test'], cohort_curves['cohort_combos'])
            st.plotly_chart(p, use_container_width=True)
            if p:
                get_download_link(p, 'pr_curve_cohort.pdf')
                get_download_link(p, 'pr_curve_cohort.svg')

        # Confusion Matrix (CM) for Cohorts
        with st.beta_expander("Confusion matrix"):
            st.subheader('Confusion matrix')
            names = ['Train on {}, Test on {}'.format(_[0], _[1]) for _ in cohort_curves['cohort_combos']]
            names.insert(0, 'Sum of cohort comparisons')

            p = plot_confusion_matrices(state.class_0, state.class_1, cohort_curves['y_hats_'], names)
            st.plotly_chart(p, use_container_width=True)
            if p:
                get_download_link(p, 'cm_cohorts.pdf')
                get_download_link(p, 'cm_cohorts.svg')

        with st.beta_expander("Table for run results"):
            state['cohort_summary'] = pd.DataFrame(pd.DataFrame(cv_results).describe())
            st.write(state.cohort_summary)
            get_download_link(state.cohort_summary, "run_results_cohort.csv")

        state['cohort_combos'] = cohort_curves['cohort_combos']
        state['cohort_results'] = cohort_results

    return state

# Main Function
def OmicLearn_Main():

    # Define state
    state = objdict()
    state['df'] = pd.DataFrame()
    state['class_0'] = None
    state['class_1'] = None

    # Main components
    widget_values, record_widgets = main_components()

    # Welcome text and Data uploading
    state = main_text_and_data_upload(state, APP_TITLE)

    # Checkpoint for whether data uploaded/selected
    state = checkpoint_for_data_upload(state, record_widgets)

    # Sidebar widgets
    #state = generate_sidebar_elements(state, icon, report, record_widgets)

    # # Analysis Part
    # if len(state.df) > 0 and state.target_column == "":
    #     st.warning('**WARNING:** Select classification target from your data.')

    # elif len(state.df) > 0 and not (state.class_0 and state.class_1):
    #     st.warning('**WARNING:** Define classes for the classification target.')

    # elif (state.df is not None) and (state.class_0 and state.class_1) and (st.button('Run analysis', key='run')):
    #     state.features = state.proteins + state.additional_features
    #     subset = state.df_sub[state.df_sub[state.target_column].isin(state.class_0) | state.df_sub[state.target_column].isin(state.class_1)].copy()
    #     state.y = subset[state.target_column].isin(state.class_0)
    #     state.X = transform_dataset(subset, state.additional_features, state.proteins)

    #     if state.cohort_column is not None:
    #         state['X_cohort'] = subset[state.cohort_column]

    #     # Show the running info text
    #     st.info(f"""
    #         **Running info:**
    #         - Using the following features: **Class 0 `{state.class_0}`, Class 1 `{state.class_1}`**.
    #         - Using classifier **`{state.classifier}`**.
    #         - Using a total of  **`{len(state.features)}`** features.
    #         - Note that OmicLearn is intended to be an exploratory tool to assess the performance of algorithms,
    #             rather than providing a classification model for production.
    #     """)

    #     # Plotting and Get the results
    #     state = classify_and_plot(state)

    #     # Generate summary text
    #     generate_text(state, report)

    #     # Session and Run info
    #     widget_values["Date"] = datetime.now().strftime("%d/%m/%Y %H:%M:%S") + " (UTC)"

    #     for _ in state.summary.columns:
    #         widget_values[_+'_mean'] = state.summary.loc['mean'][_]
    #         widget_values[_+'_std'] = state.summary.loc['std'][_]

    #     user_name = str(random.randint(0, 10000)) + "OmicLearn"
    #     session_state = session_states.get(user_name=user_name)
    #     widget_values["user"] = session_state.user_name
    #     widget_values["top_features"] = state.top_features
    #     save_sessions(widget_values, session_state.user_name)

    #     # Generate footer
    #     generate_footer_parts(report)
    # else:
    #     pass

# Run the OmicLearn
if __name__ == '__main__':
    try:
        OmicLearn_Main()
    except (ValueError, IndexError) as val_ind_error:
        st.error(f"There is a problem with values/parameters or dataset due to {val_ind_error}.")
    except TypeError as e:
        # st.warning("TypeError exists in {}".format(e))
        pass
